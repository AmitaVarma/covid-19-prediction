{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image data\n",
    "\n",
    "data = []\n",
    "file_list = []\n",
    "y = []\n",
    "\n",
    "for png in os.listdir(\"data/CT_COVID\"):\n",
    "    pic = plt.imread(\"data/CT_COVID/\" + png, 0)\n",
    "    data.append(pic)\n",
    "    file_list.append(png)\n",
    "    y.append(1)\n",
    "\n",
    "for png in os.listdir(\"data/CT_NonCOVID\"):\n",
    "    pic = plt.imread(\"data/CT_NonCOVID/\" + png, 0)\n",
    "    data.append(pic)\n",
    "    file_list.append(png)\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(\"data/COVID-CT-MetaInfo.xlsx\", header = None, names = ['file','patient','column3','note'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = (300, 400) #use the averages for the dimensions\n",
    "\n",
    "data_cleaned = []\n",
    "\n",
    "for img in data:\n",
    "    #First, take the mean of the 3rd dimension (channels) if it exists\n",
    "    if len(img.shape) == 3:\n",
    "        img = np.mean(img, axis = 2)\n",
    "    \n",
    "    #Using PIL Image processor, resize using high quality down-sampling filter \n",
    "    img = Image.fromarray(img)\n",
    "    img = img.resize((image_dim[1], image_dim[0]), Image.ANTIALIAS)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    #Normalize image values\n",
    "    img = img/255\n",
    "    \n",
    "    data_cleaned.append(img)\n",
    "\n",
    "data_cleaned = np.array(data_cleaned)\n",
    "data_cleaned = np.expand_dims(data_cleaned,axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, y_train, y_val = train_test_split(data_cleaned, np.array(y),\\\n",
    "                                                        train_size = 0.75, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flip horizontally\n",
    "horiz_flip = tf.image.flip_left_right(data_train) \n",
    "#Flip vertically\n",
    "vert_flip = tf.image.flip_up_down(data_train)\n",
    "\n",
    "data_train_augmented = np.concatenate((data_train, horiz_flip, vert_flip), axis = 0)\n",
    "y_train_augmented = np.concatenate((y_train,y_train,y_train), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG and concatenate input images into 3 channels because vgg takes rgb images\n",
    "img_input = Input(shape=(300, 400, 1))\n",
    "model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Concatenate()([img_input, img_input, img_input]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer and a dense layer to classify 2 classes\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# new model to train\n",
    "new_model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "# freeze all convolutional VGG layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 300, 400, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300, 400, 3)  0           input_2[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 300, 400, 64) 1792        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 300, 400, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 150, 200, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 150, 200, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 150, 200, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 75, 100, 128) 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 75, 100, 256) 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 75, 100, 256) 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 75, 100, 256) 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 37, 50, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 37, 50, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 37, 50, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 37, 50, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 18, 25, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 18, 25, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 18, 25, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 18, 25, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 9, 12, 512)   0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 14,715,714\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1677 samples, validate on 187 samples\n",
      "Epoch 1/10\n",
      "1677/1677 [==============================] - 41s 25ms/sample - loss: 0.6829 - accuracy: 0.5593 - val_loss: 0.6738 - val_accuracy: 0.6150\n",
      "Epoch 2/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6759 - accuracy: 0.5868 - val_loss: 0.6646 - val_accuracy: 0.6524\n",
      "Epoch 3/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6641 - accuracy: 0.6124 - val_loss: 0.6594 - val_accuracy: 0.5829\n",
      "Epoch 4/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6571 - accuracy: 0.6333 - val_loss: 0.6544 - val_accuracy: 0.6310\n",
      "Epoch 5/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6495 - accuracy: 0.6428 - val_loss: 0.6414 - val_accuracy: 0.6364\n",
      "Epoch 6/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6408 - accuracy: 0.6452 - val_loss: 0.6332 - val_accuracy: 0.7326\n",
      "Epoch 7/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6320 - accuracy: 0.6738 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 8/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6257 - accuracy: 0.7078 - val_loss: 0.6215 - val_accuracy: 0.6578\n",
      "Epoch 9/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6204 - accuracy: 0.6983 - val_loss: 0.6113 - val_accuracy: 0.7540\n",
      "Epoch 10/10\n",
      "1677/1677 [==============================] - 40s 24ms/sample - loss: 0.6144 - accuracy: 0.7114 - val_loss: 0.6099 - val_accuracy: 0.7326\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=5e-4)\n",
    "new_model.compile(optimizer=sgd, loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "history = new_model.fit(\n",
    "        data_train_augmented,\n",
    "        tf.keras.utils.to_categorical(y_train_augmented),\n",
    "        epochs=10,\n",
    "        batch_size = 16, \n",
    "        validation_data=(data_val, tf.keras.utils.to_categorical(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next to try: instead of replicating 3 channels use different rescalings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
